tmp_3 <- cbind(tmp_1,tmp_2)
tmp_3$cvtd_timestamp <- tmp_3 $cvtd_timestamp %>% parse_date_time(orders="%d/%m/%y %H:%M")
tmp_3$new_window <- ifelse(as.character(tmp_3 $new_window)=="yes",TRUE,FALSE)
return(tmp_3)
}
df_clean <- cbind(classe=y,CleanTheData(df))
df_clean %>% dim()
head(df_clean)
df_drop <- whichAreInDouble(df,verbose=FALSE)
df_clean[,df_drop] <- NULL
df_clean %>% dim()
FilterTheData <- function(x,perc=0.5){
col_to_remove <- x %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame() %>% rename(Percentage=V1) %>%  subset(Percentage>perc) %>% row.names()
x[,col_to_remove ] <- NULL
return(x)
}
df_filtered <- FilterTheData(df_clean,0.7)
complete.cases(df_filtered)
complete.cases(df_filtered) %>% table()
complete.cases(df_filtered) %>% table()
df_drop <- complete.cases(df_filtered)
df_filtered[df_drop,] <- NULL
complete.cases(df_filtered) %>% table()
na.omit(df_filtered)
complete.cases(df_filtered) %>% table()
df_filtered <- na.omit(df_filtered)
complete.cases(df_filtered) %>% table()
set.seed(17032020)
train_index <- createDataPartition(df_filtered$classe, p=0.6, list=FALSE)
train <- df_filtered[train_index,]
test <- df_filtered[-train_index,]
test_index <- createDataPartition(test$classe, p=0.7, list=FALSE)
validation <- test[-test_index ,]
test <- test[test_index ,]
# training.subSetTrain <- training.cleaned[partition, ]
# training.subSetTest <- training.cleaned[-partition, ]
train_numeric <- train[,4:ncol(train)]
correlations <- cor(train_numeric)
View(train)
View(train)
train_numeric <- train[,6:ncol(train)]
correlations <- cor(train_numeric)
test_numeric <- test[,6:ncol(test)]
validation_numeric <- validation[,6:ncol(test)]
pca_schema <- preProcess(train_numeric, method = "pca")
train_pca <- predict(pca_schema , train_numeric) %>% cbind(.,classe=train[,1])
test_pca <- predict(pca_schema , test_numeric) %>% cbind(.,classe=test[,1])
validation_pca <- predict(pca_schema , validation_numeric) %>% cbind(.,classe=validation[,1])
View(train_pca)
View(train)
train_numeric <- train[,7:ncol(train)]
correlations <- cor(train_numeric)
test_numeric <- test[,7:ncol(test)]
validation_numeric <- validation[,7:ncol(test)]
pca_schema <- preProcess(train_numeric, method = "pca")
train_pca <- predict(pca_schema , train_numeric) %>% cbind(.,classe=train[,1])
test_pca <- predict(pca_schema , test_numeric) %>% cbind(.,classe=test[,1])
validation_pca <- predict(pca_schema , validation_numeric) %>% cbind(.,classe=validation[,1])
View(train_pca)
controlXGB <- trainControl(method="cv", 5, allowParallel = TRUE)
modelXGB <- train(classe ~ ., data=train_pca, method="xgbTree", trControl=controlXGB)
library(xgboost)
controlXGB <- trainControl(method="cv", 5, allowParallel = TRUE)
modelXGB <- train(classe ~ ., data=train_pca, method="xgbTree", trControl=controlXGB)
n_folds <- 7
ntree <- 100
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(x_train$classe ~ ., data=x_train, method="rf", trControl=controlRf, ntree)
#tmp_predict <- predict(tmp_model, x_test)
return(modelRf )
}
RF_model <- applyRF(train_pca,nfolds=7,ntree=3)
controlRf <- trainControl(method="cv", number=n_folds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=training_pca, method="rf", trControl=controlRf, ntree=3)
controlRf <- trainControl(method="cv", number=n_folds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree=3)
predictRf <- predict(modelRf, testing_pca)
predictRf <- predict(modelRf, test_pca)
confusionMatrix(validation_pca$classe, predictRf)
confusionMatrix(test_pca$classe, predictRf)
controlRf <- trainControl(method="cv", number=n_folds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree=10)
predictRf <- predict(modelRf, test_pca)
confusionMatrix(test_pca$classe, predictRf)
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(x_train$classe ~ ., data=x_train, method="rf", trControl=controlRf, ntree)
#tmp_predict <- predict(tmp_model, x_test)
return(modelRf )
}
RF_model <- applyRF(train_pca,nfolds=7,ntree=3)
RF_model <- applyRF(train_pca,nfolds=7,ntree=10)
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
#tmp_predict <- predict(tmp_model, x_test)
return(modelRf )
}
RF_model <- applyRF(train_pca,nfolds=7,ntree=10)
confusionMatrix(test_pca$classe, predictRf) %>% str()
confusionMatrix(test_pca$classe, predictRf)[, 'class.error'])
confusionMatrix(test_pca$classe, predictRf)[, 'class.error']
overall
confusionMatrix(test_pca$classe, predictRf)overall
confusionMatrix(test_pca$classe, predictRf)$overall
confusionMatrix(test_pca$classe, predictRf)$overall$Accuracy
confusionMatrix(test_pca$classe, predictRf)$overall[1]
results=data.frame()
results <- confusionMatrix(test_pca$classe, predictRf)$overall %>%  as.data.frame() %>% t()
View(results)
results <- confusionMatrix(test_pca$classe, predictRf)$overall %>%  as.data.frame()
View(results)
?seq
searchGrid <- expand.grid(n_folds = 4:10,n_tree = seq(50,100,10))
ntree <- 100
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
View(searchGrid)
controlRf <- trainControl(method="cv", number=n_folds, allowParallel = TRUE)
modelRf <- train(train_pca$classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
controlRf <- trainControl(method="cv", number=n_folds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
controlRf <- trainControl(method="cv", number=n_folds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree=10)
predictRf <- predict(modelRf, test_pca)
searchGrid <- expand.grid(n_folds = 4:10,n_tree = seq(5,10,1))
ntree <- 100
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
searchGrid <- expand.grid(n_folds = 4:10,n_tree = seq(5,10,1))
ntree <- 100
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
View(results)
View(searchGrid)
apply(searchGrid,1,function(grid_sample){
nfolds <- grid_sample[1]
ntree <- grid_sample[2]
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
predictRf <- predict(modelRf, test_pca)})
apply(searchGrid,1,function(grid_sample){
nfolds <- grid_sample[1]
ntree <- grid_sample[2]
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
# modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
# predictRf <- predict(modelRf, test_pca)
})
apply(searchGrid,1,function(grid_sample){
nfolds <- grid_sample[1]
ntree <- grid_sample[2]
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
# predictRf <- predict(modelRf, test_pca)
})
apply(searchGrid,1,function(grid_sample){
nfolds <- grid_sample[1]
ntree <- grid_sample[2]
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
nfolds
#modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
# predictRf <- predict(modelRf, test_pca)
})
apply(searchGrid,1,function(grid_sample){
nfolds <- grid_sample[1]
ntree <- grid_sample[2]
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
ntree
#modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
# predictRf <- predict(modelRf, test_pca)
})
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, ntree)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf,10)
controlRf <- trainControl(method="cv", number=4, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf,10)
controlRf <- trainControl(method="cv", number=5, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf,10)
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
controlRf <- trainControl(method="cv", number=7, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf, 10)
?train
?trainControl
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf)
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv",metric="Accuracy", number=7, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf", trControl=controlRf)
#tmp_predict <- predict(tmp_model, x_test)
return(modelRf )
}
RF_model <- applyRF(train_pca,nfolds=7)
controlRf <- trainControl(method="cv",metric="Accuracy", number=7, allowParallel = TRUE)
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv", number=7, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf",,metric="Accuracy" trControl=controlRf)
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv", number=7, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf",metric="Accuracy",trControl=controlRf)
#tmp_predict <- predict(tmp_model, x_test)
return(modelRf )
}
RF_model <- applyRF(train_pca,nfolds=7)
applyRF <- function(x_train,nfolds,ntree){
controlRf <- trainControl(method="cv", number=7, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf",metric="Accuracy",trControl=controlRf)
return(modelRf )
}
searchGrid <- expand.grid(n_folds = 4:10,spec_param = seq(5,10,1))
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
applyRF <- function(x_train,x_test,method,nfolds,spec_param){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf",metric="Accuracy",trControl=controlRf,mtry=spec_param)
return(modelRf )
}
results <- applyRF(searchGrid,1,train_pca,nfolds=7)
applyRF <- function(x_train,x_test,method,nfolds=5,spec_param=10){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca, method="rf",metric="Accuracy",trControl=controlRf,mtry=spec_param)
return(modelRf )
}
results <- applyRF(searchGrid,1,train_pca,nfolds=7)
applyRF <- function(x_train,nfolds=5,spec_param=10){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca,
method="rf",metric="Accuracy",trControl=controlRf,mtry=spec_param)
return(modelRf )
}
View(searchGrid)
applyRF <- function(x_train,nfolds=5,spec_param=10){
controlRf <- trainControl(method="cv", number=nfolds, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca,
method="rf",metric="Accuracy",trControl=controlRf,mtry=spec_param)
return(modelRf )
}
searchGrid <- expand.grid(n_folds = 4:10,spec_param = seq(5,10,1))
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
results <- applyRF(searchGrid,1,FUN=function(x){
model <- applyRF(train_pca,x[1],x[2])
})
results <- apply(searchGrid,1,FUN=function(x){
model <- applyRF(train_pca,x[1],x[2])
})
controlRf <- trainControl(method="cv", number=3, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca,method="rf",metric="Accuracy",trControl=controlRf,mtry=5)
View(train_pca)
controlRf <- trainControl(method="cv", number=3, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca,method="RRF",metric="Accuracy",trControl=controlRf)
controlRf <- trainControl(method="cv", number=3, allowParallel = TRUE)
modelRf <- train(classe ~ ., data=train_pca,method="RRF",metric="Accuracy",trControl=controlRf)
searchGrid <- expand.grid(nleaves = 4:10,n_trees= seq(5,10,1))
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
?trainControl
results <- apply(searchGrid,1,FUN=function(x){
n_leaves <- x[1]
n_trees <- x[2]
model <- train(classe ~ ., data=train_pca,
method="logicBag",metric="Accuracy",n_leaves,n_trees)
results <- confusionMatrix(test_pca$classe, predict(model,test_pca))$overall %>%  as.data.frame()
searchGrid <- expand.grid(nleaves = 4:10,n_trees= seq(5,10,1))
#validation_pca %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame()
results <- apply(searchGrid,1,FUN=function(x){
n_leaves <- x[1]
n_trees <- x[2]
model <- train(classe ~ ., data=train_pca,
method="logicBag",metric="Accuracy",n_leaves,n_trees)
results <- confusionMatrix(test_pca$classe, predict(model,test_pca))$overall %>%  as.data.frame()
})
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install(version = "3.10")
results <- apply(searchGrid,1,FUN=function(x){
n_leaves <- x[1]
n_trees <- x[2]
model <- train(classe ~ ., data=train_pca,
method="logicBag",metric="Accuracy",n_leaves,n_trees)
results <- confusionMatrix(test_pca$classe, predict(model,test_pca))$overall %>%  as.data.frame()
})
install.packages("BiocManager")
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install(version = "3.10")
install.packages(c("dplyr", "glue", "ModelMetrics", "Rcpp"))
install.packages(c("dplyr", "glue", "ModelMetrics", "Rcpp"))
install.packages(c("dplyr", "glue", "ModelMetrics", "Rcpp"))
install.packages("dplyr")
install.packages("RTools")
install.packages("Rtools")
install.packages("Rcpp")
install.packages("Rcpp")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(lubridate)
library(caret)
library(dataPreparation)
library(ggcorrplot)
library(GGally)
library(readr)
library(xgboost)
setwd("C:/Users/theko/Google Drive/Corsi/CourseMaterial/Coursera/WIP/Data Science_ Statistics and Machine Learning Specialization/03_PracticalMachineLearning")
df <- read.csv("C:/Users/theko/Google Drive/Corsi/CourseMaterial/Coursera/WIP/Data Science_ Statistics and Machine Learning Specialization/03_PracticalMachineLearning/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv")
str(df,list.len=15)
y <- df$classe
df$classe <- NULL
CleanTheData <- function(x){
tmp_1 <- x[,1:5]
tmp_2 <- x[,6:ncol(x)]
tmp_2 <- apply(tmp_2,2,FUN=function(y){as.numeric(as.character(y))})
tmp_3 <- cbind(tmp_1,tmp_2)
tmp_3$cvtd_timestamp <- tmp_3 $cvtd_timestamp %>% parse_date_time(orders="%d/%m/%y %H:%M")
tmp_3$new_window <- ifelse(as.character(tmp_3 $new_window)=="yes",TRUE,FALSE)
return(tmp_3)
}
df_clean <- cbind(classe=y,CleanTheData(df))
df_clean %>% dim()
head(df_clean)
df_drop <- whichAreInDouble(df,verbose=FALSE)
df_clean[,df_drop] <- NULL
df_clean %>% dim()
FilterTheData <- function(x,perc=0.5){
col_to_remove <- x %>% summarise_all(funs(sum(is.na(.)) / length(.))) %>% t() %>% as.data.frame() %>% rename(Percentage=V1) %>%  subset(Percentage>perc) %>% row.names()
x[,col_to_remove ] <- NULL
return(x)
}
df_filtered <- FilterTheData(df_clean,0.7)
unwanted_col = grepl("timestamp", colnames(df_filtered))
unwanted_col = colnames(df_filtered)[unwanted_col]
df_filtered[,unwanted_col] <- NULL
complete.cases(df_filtered) %>% table()
df_filtered <- na.omit(df_filtered)
set.seed(17032020)
train_index <- createDataPartition(df_filtered$classe, p=0.6, list=FALSE)
train <- df_filtered[train_index,]
test <- df_filtered[-train_index,]
test_index <- createDataPartition(test$classe, p=0.7, list=FALSE)
validation <- test[-test_index ,]
test <- test[test_index ,]
# training.subSetTrain <- training.cleaned[partition, ]
# training.subSetTest <- training.cleaned[-partition, ]
ggplot(train,aes(y=user_name))+geom_histogram(stat="count",aes(fill=classe))
train_numeric <- train[,7:ncol(train)]
correlations <- cor(train_numeric)
ggcorrplot(correlations)+ theme(axis.text.x= element_text( size=6,angle=90))+theme(axis.text.y= element_text( size=7,angle=0))
test_numeric <- test[,7:ncol(test)]
validation_numeric <- validation[,7:ncol(test)]
pca_schema <- preProcess(train_numeric, method = "pca")
train_pca <- predict(pca_schema , train_numeric) %>% cbind(.,classe=train[,1])
test_pca <- predict(pca_schema , test_numeric) %>% cbind(.,classe=test[,1])
validation_pca <- predict(pca_schema , validation_numeric) %>% cbind(.,classe=validation[,1])
install.packages("paramtest")
library(paramtest)
classifier = function(iter,df, treesize,ntrees){
model <- train(classe = . ~ ., data = x, method = 'logreg',treesize,ntrees)
return(model)}
tmp <- grid_search(classifier,list(treesize=seq(10,50,10),ntrees=1:10))
tmp <- grid_search(classifier,list(df=train_pca,treesize=seq(10,50,10),ntrees=1:10))
tmp <- grid_search(classifier,list(treesize=seq(10,50,10),ntrees=1:10),n.iter=100,df=train_pca)
classifier = function(iter,df, treesize,ntrees){
model <- train(classe = . ~ ., data = df, method = 'logreg',treesize,ntrees)
return(model)}
tmp <- grid_search(classifier,list(treesize=seq(10,50,10),ntrees=1:10),n.iter=100,df=train_pca)
View(train_pca)
classifier = function(iter,df, treesize,ntrees){
model <- train(classe  ~ ., data = df, method = 'logreg',treesize,ntrees)
return(model)}
tmp <- grid_search(classifier,list(treesize=seq(10,50,10),ntrees=1:10),n.iter=100,df=train_pca)
tmp <- grid_search(classifier,list(treesize=1:6,ntrees=1:10),n.iter=100,df=train_pca)
lm_test <- function(iter, data, b0, b1) {
model <- lm(y ~ x, data)
# capture output from model summary
est <- coef(summary(model))['x', 'Estimate']
se <- coef(summary(model))['x', 'Std. Error']
p <- coef(summary(model))['x', 'Pr(>|t|)']
return(c(xm=mean(x), xsd=sd(x), ym=mean(y), ysd=sd(y), est=est, se=se, p=p,
sig=est > 0 & p <= .05))
}
x <- rnorm(N, 0, 1)
y <- rnorm(N, b0 + b1*x, sqrt(1 - b1^2))
data <- data.frame(y, x)
lm_test <- function(iter, data, b0, b1) {
model <- lm(y ~ x, data)
# capture output from model summary
est <- coef(summary(model))['x', 'Estimate']
se <- coef(summary(model))['x', 'Std. Error']
p <- coef(summary(model))['x', 'Pr(>|t|)']
return(c(xm=mean(x), xsd=sd(x), ym=mean(y), ysd=sd(y), est=est, se=se, p=p,
sig=est > 0 & p <= .05))
}
# test power for sample size N=200 and N=300, with 500 iterations for each
power_sim <- grid_search(lm_test, params=list(N=c(200, 300)), n.iter=500, b0=0, b1=.15)
x <- rnorm(N, 0, 1)
y <- rnorm(N, b0 + b1*x, sqrt(1 - b1^2))
data <- data.frame(y, x)
lm_test <- function(iter, data, b0, b1) {
model <- lm(y ~ x, data)
# capture output from model summary
est <- coef(summary(model))['x', 'Estimate']
se <- coef(summary(model))['x', 'Std. Error']
p <- coef(summary(model))['x', 'Pr(>|t|)']
return(c(xm=mean(data$x), xsd=sd(data$x), ym=mean(data$y), ysd=sd(data$y), est=est, se=se, p=p,
sig=est > 0 & p <= .05))
}
# test power for sample size N=200 and N=300, with 500 iterations for each
power_sim <- grid_search(lm_test, params=list(data=data), n.iter=500, b0=0, b1=.15)
x <- rnorm(N, 0, 1)
y <- rnorm(N, b0 + b1*x, sqrt(1 - b1^2))
data <- data.frame(y, x)
lm_test <- function(iter, df, b0, b1) {
model <- lm(y ~ x, df)
# capture output from model summary
est <- coef(summary(model))['x', 'Estimate']
se <- coef(summary(model))['x', 'Std. Error']
p <- coef(summary(model))['x', 'Pr(>|t|)']
return(c(xm=mean(df$x), xsd=sd(df$x), ym=mean(df$y), ysd=sd(df$y), est=est, se=se, p=p,
sig=est > 0 & p <= .05))
}
# test power for sample size N=200 and N=300, with 500 iterations for each
power_sim <- grid_search(lm_test, params=list(df=data), n.iter=500)
install.packages("NMOF")
library(NMOF)
?predict
?confusionMatrix
?caret::confusionMatrix
class_pred = function(iter,df_train,df_test, treesize,ntrees){
model <- train(classe  ~ ., data = df_train, method = 'logreg',treesize,ntrees)
pred <- predict(model,df_test)
CM <- caret::confusionMatrix(df_test$actual,pred)$overall %>% as.data.frame()
return(CM)}
grid <- expand.grid(treesize=5:10, ntrees=10:20)
model <- train(classe  ~ ., data = train_pca, method = 'logreg',tuneGrid=grid)
str(train_pca)
grid <- expand.grid(mtry=5:10)
model <- train(classe  ~ ., data = train_pca, method = 'qrf',tuneGrid=grid)
grid <- expand.grid(mtry=5:10)
model <- train(classe  ~ ., data = train_pca, method = 'rf',tuneGrid=grid)
control <- trainControl(method="cv", 5, allowParallel = TRUE)
model <- train(classe  ~ ., data = train_pca, method = 'rf',tuneGrid=grid,trControl=control)
#pred <- predict(model,df_test)
#CM <- caret::confusionMatrix(df_test$actual,pred)$overall %>% as.data.frame()
View(model)
pred <-lapply(model,FUN=function(x){predict(x,test_pca$classe)})
model[[1]]
model[1]
model$fit[1]
model$fit
str(model)
plot(model)
?trainControl
model$results
whichTwoPct <- tolerance(model$results, metric = "ROC",
tol = 2, maximize = TRUE)
whichTwoPct <- tolerance(model$results, metric = "Accuracy",
tol = 2, maximize = TRUE)
cat("best model within 2 pct of best:\n")
model$results[whichTwoPct,1:6]
model$results
pred <- predict(model,df_test)
pred <- predict(model,test_pca)
pred
pred <- predict(model,test_pca)
CM <- caret::confusionMatrix(df_test$actual,pred)$overall %>% as.data.frame()
pred <- predict(model,test_pca)
CM <- caret::confusionMatrix(test_pca$actual,pred)$overall %>% as.data.frame()
len(pred)
dim(pred)
length(pred)
pred <- predict(model,test_pca)
CM <- caret::confusionMatrix(test_pca$actual,pred)#$overall %>% as.data.frame()
test_pca$actual
pred <- predict(model,test_pca)
CM <- caret::confusionMatrix(test_pca$classe,pred)#$overall %>% as.data.frame()
pred <- predict(model,test_pca)
CM <- caret::confusionMatrix(test_pca$classe,pred)$overall %>% as.data.frame()
View(CM)
grid <- expand.grid(mtry=5:10)
CM <- data.frame()
pred <- predict(model,test_pca)
CM$RandomForest <- caret::confusionMatrix(test_pca$classe,pred)$overall %>% as.data.frame()
caret::confusionMatrix(test_pca$classe,pred)$overall %>% as.data.frame()
grid <- expand.grid(mtry=5:10)
CM <- data.frame()
pred <- predict(model,test_pca)
CM$RandomForest <- caret::confusionMatrix(test_pca$classe,pred)$overall %>% as.data.frame()
setwd("C:/Users/theko/Google Drive/Corsi/CourseMaterial/Coursera/WIP/Data Science_ Statistics and Machine Learning Specialization/03_PracticalMachineLearning/Assignment")
install.packages("janitor")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(lubridate)
library(caret)
library(dataPreparation)
library(ggcorrplot)
library(GGally)
library(readr)
library(janitor)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(lubridate)
library(caret)
library(dataPreparation)
library(ggcorrplot)
library(GGally)
library(readr)
library(janitor)
setwd("C:/Users/theko/Google Drive/Corsi/CourseMaterial/Coursera/WIP/Data Science_ Statistics and Machine Learning Specialization/03_PracticalMachineLearning/Assignment")
df <- read_csv("pml-training.csv")
df_quiz <- read_csv("pml-testing.csv")
df[,1] <-NULL
df_quiz[,1] <-NULL
unlink('Assignmnet_cache', recursive = TRUE)
